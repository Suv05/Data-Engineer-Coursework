# ğŸ§  Retail Orders Data Engineering & Analytics Project (Databricks)

This project demonstrates a complete data engineering and analytics pipeline using **Apache Spark in Databricks**. The goal was to simulate a **real-world retail data workflow**, including:

- Data ingestion
- Data validation
- Cleaning and transformation
- Efficient storage
- Advanced visual analytics

All tasks were performed **entirely within the Databricks environment**, making use of PySpark and interactive dashboards.

---

## ğŸ“¦ Dataset Overview

The dataset mimics a **retail e-commerce orders system** with the following features:

- 25+ columns including order metadata, customer segmentation, product pricing, discounts, promotions, delivery dates, and ratings.
- Complexities such as:
  - Missing and null values
  - Invalid formats (e.g., `INVALID_DATE`)
  - Duplicate rows
  - Inconsistent schemas (booleans as strings, negative prices, etc.)

---

## ğŸ”„ Project Workflow

### âœ… 1. Data Ingestion
- Imported raw CSV into Databricks using the databricks/volume.
- StructType schema and read as a Spark DataFrame.

### âœ… 2. Data Validation
- Detected missing/null fields across critical columns.
- Checked for:
  - Negative or zero pricing
  - Out-of-bound ratings
  - Invalid dates
  - Duplicated rows
  - Blank or NULL `ProductID`, `CustomerID`, `OrderDate`

### âœ… 3. Data Cleaning & Transformation
- Cleaned and cast date fields to `DateType`.
- Replaced invalid/NULL entries with default values or averages.
- Standardized boolean fields (`TRUE` / `FALSE` â†’ `True` / `False`).
- Calculated new fields:
  - `TotalPrice = Quantity * UnitPrice`
  - `DeliveryTimeDays = DeliveryDate - OrderDate`
  - `IsDelayedDelivery`
- Extracted time-based features (Year, Month).

### âœ… 4. AGGREGATIONS AND BUSINESS METRICS
- SALES PERFORMANCE ANALYSIS
- Sales by country
- Product Performance Analysis
- Customer Segment Analysis
- Payment Method Analysis

### âœ… 5. Data Storage
- Saved the cleaned and transformed dataset in **Parquet format** for optimized performance and storage.

### âœ… 6. Interactive Dashboards (Visual Analytics)
Used Databricks dashboards to visualize insights and monitor key performance metrics:

---

## ğŸ“Š Dashboards & Insights

### ğŸ§® Customer Lifetime Value vs Number of Orders
![image](https://github.com/user-attachments/assets/7e4710bc-d19c-455c-b4c2-523731afc98e)


---

### ğŸšš Shipping Status Distribution
![image](https://github.com/user-attachments/assets/3b8def00-f302-4e18-b097-4ebf0f087dfd)


---

### ğŸŒ Shipping Performance by Country and Status
![image](https://github.com/user-attachments/assets/c16cd71d-75a0-4ec9-895f-6361b2c4954f)


---

### ğŸ” Customer Order Frequency Distribution
![image](https://github.com/user-attachments/assets/30fc4768-2714-44bf-a49a-bb91d349abf3)


---

### ğŸ“… Daily Sales with 7-Day Moving Average
![Daily sales with 7-day moving average](/image/image-4.png)

---

### ğŸ’° Revenue by Customer Segment
![Revenue by customer segment](/image/image-5.png)

---

### ğŸŒ Order Distribution by Country
![Order distribution By country](/image/image-6.png)

---

### ğŸ“ˆ Order Trends Over Time
![Order trends over time](/image/image-7.png)

---

### ğŸ•’ Daily Sales (Smoothed View)
![image](https://github.com/user-attachments/assets/4a67ec19-192e-4914-a01a-c75e55362a9d)


---

## ğŸ› ï¸ Tools Used
- **Apache Spark (PySpark)** â€” scalable data processing
- **Databricks Notebook** â€” unified development environment
- **Databricks SQL & Dashboards** â€” for visual analytics
- **Parquet Format** â€” optimized data storage

---

## ğŸ Outcomes
- ğŸš€ Complete hands-on practice in a near-production setup
- âœ… Cleaned & transformed retail dataset using PySpark
- ğŸ“ˆ Multiple business insights generated via dashboards
- ğŸ”„ Ready for integration with BigQuery or Power BI

---

## ğŸ“š Future Scope
- Integrate BigQuery for warehousing
- Build Power BI reports using cleaned data
- Extend dataset to support ML (churn prediction, CLTV modeling)
