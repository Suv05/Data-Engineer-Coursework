{"cells": [{"cell_type": "markdown", "id": "f1fb8b22-7dff-46a2-a527-a262e7812d2c", "metadata": {}, "source": "# Spark SQL DataFrames - Learn by Coding Tutorial"}, {"cell_type": "markdown", "id": "37d4cb09-87b4-4040-ac79-7dfdc609366d", "metadata": {}, "source": "## Setup and Environment"}, {"cell_type": "code", "execution_count": 9, "id": "4f9effe8-43c3-4a09-9315-27d96b9946ac", "metadata": {"tags": []}, "outputs": [], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *"}, {"cell_type": "code", "execution_count": 10, "id": "89b7eb3b-a06a-4847-8deb-078350b46b0e", "metadata": {"tags": []}, "outputs": [], "source": "# \ud83d\udd27 SOLUTION 1: Check if Spark session exists, create if needed\ndef get_spark_session():\n    try:\n        # Try to get existing active session\n        spark = SparkSession.getActiveSession()\n        if spark is None:\n            # Create new session if none exists\n            spark = SparkSession.builder \\\n                .appName(\"SparkSQL_Learning\") \\\n                .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n                .getOrCreate()\n        return spark\n    except:\n        # If anything goes wrong, create fresh session\n        spark = SparkSession.builder \\\n            .appName(\"SparkSQL_Learning\") \\\n            .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n            .getOrCreate()\n        return spark"}, {"cell_type": "code", "execution_count": 11, "id": "c84e6d82-bc69-4136-846d-33d2fa0d3723", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "25/06/26 14:05:08 INFO SparkEnv: Registering MapOutputTracker\n25/06/26 14:05:08 INFO SparkEnv: Registering BlockManagerMaster\n25/06/26 14:05:08 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n25/06/26 14:05:09 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "# Get or create Spark session\nspark = get_spark_session()\nsc = spark.sparkContext"}, {"cell_type": "code", "execution_count": 12, "id": "8f4a80bc-9911-48d2-9f85-c7abd5d5832e", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "\u2705 Spark Session Status:\n   App Name: SparkSQL_Learning\n   Master: yarn\n   Spark Version: 3.5.3\n   Active: True\n"}], "source": "print(\"\u2705 Spark Session Status:\")\nprint(f\"   App Name: {spark.sparkContext.appName}\")\nprint(f\"   Master: {spark.sparkContext.master}\")\nprint(f\"   Spark Version: {spark.version}\")\nprint(f\"   Active: {not spark.sparkContext._jsc.sc().isStopped()}\")"}, {"cell_type": "markdown", "id": "661a7886-bc4f-4d22-bdb9-00869f7f48f2", "metadata": {}, "source": "## 1. Creating DataFrames"}, {"cell_type": "markdown", "id": "aca17035-01a9-4d91-8f0d-badef7497934", "metadata": {}, "source": "### From Lists/Tuples"}, {"cell_type": "code", "execution_count": 13, "id": "6140de7e-5009-4a18-a3be-fe9d3ea04e2c", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+---+-----+-----------+------+----------+\n| id| name| department|salary| hire_date|\n+---+-----+-----------+------+----------+\n|  1| John|Engineering| 75000|2020-01-15|\n|  2|Sarah|  Marketing| 65000|2019-03-22|\n|  3| Mike|Engineering| 80000|2021-06-10|\n|  4| Lisa|         HR| 60000|2018-11-05|\n|  5|David|Engineering| 85000|2022-02-28|\n+---+-----+-----------+------+----------+\n\n"}], "source": "# Sample data\nemployees_data = [\n    (1, \"John\", \"Engineering\", 75000, \"2020-01-15\"),\n    (2, \"Sarah\", \"Marketing\", 65000, \"2019-03-22\"),\n    (3, \"Mike\", \"Engineering\", 80000, \"2021-06-10\"),\n    (4, \"Lisa\", \"HR\", 60000, \"2018-11-05\"),\n    (5, \"David\", \"Engineering\", 85000, \"2022-02-28\")\n]\n\n# Define schema\nschema = StructType([\n    StructField(\"id\", IntegerType(), True),\n    StructField(\"name\", StringType(), True),\n    StructField(\"department\", StringType(), True),\n    StructField(\"salary\", IntegerType(), True),\n    StructField(\"hire_date\", StringType(), True)\n])\n\n# Create DataFrame\ndf_employees = spark.createDataFrame(employees_data, schema)\ndf_employees.show()"}, {"cell_type": "markdown", "id": "9fde2481-239d-49ae-b064-509359855d6b", "metadata": {}, "source": "### From Dictionary"}, {"cell_type": "code", "execution_count": 14, "id": "6d1e5849-ebd1-4a9b-a3ce-4a8503a81aae", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-----------+---+-----+------+\n| department| id| name|salary|\n+-----------+---+-----+------+\n|Engineering|  1| John| 75000|\n|  Marketing|  2|Sarah| 65000|\n|Engineering|  3| Mike| 80000|\n+-----------+---+-----+------+\n\n"}], "source": "# Alternative way using dictionary\nemployees_dict = [\n    {\"id\": 1, \"name\": \"John\", \"department\": \"Engineering\", \"salary\": 75000},\n    {\"id\": 2, \"name\": \"Sarah\", \"department\": \"Marketing\", \"salary\": 65000},\n    {\"id\": 3, \"name\": \"Mike\", \"department\": \"Engineering\", \"salary\": 80000}\n]\n\ndf_from_dict = spark.createDataFrame(employees_dict)\ndf_from_dict.show()"}, {"cell_type": "markdown", "id": "9c06fb35-bc57-44f9-a6f7-7b83f75b91a9", "metadata": {}, "source": "## 2. Basic DataFrame Operations"}, {"cell_type": "markdown", "id": "fd1b2692-4330-48c9-bde5-a814818e3b07", "metadata": {}, "source": "### Viewing Data"}, {"cell_type": "code", "execution_count": 15, "id": "16f0aa75-f22b-46e4-8e2a-486febe8ac53", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----+-----------+------+----------+\n| id| name| department|salary| hire_date|\n+---+-----+-----------+------+----------+\n|  1| John|Engineering| 75000|2020-01-15|\n|  2|Sarah|  Marketing| 65000|2019-03-22|\n|  3| Mike|Engineering| 80000|2021-06-10|\n|  4| Lisa|         HR| 60000|2018-11-05|\n|  5|David|Engineering| 85000|2022-02-28|\n+---+-----+-----------+------+----------+\n\n+---+-----+-----------+------+----------+\n| id| name| department|salary| hire_date|\n+---+-----+-----------+------+----------+\n|  1| John|Engineering| 75000|2020-01-15|\n|  2|Sarah|  Marketing| 65000|2019-03-22|\n|  3| Mike|Engineering| 80000|2021-06-10|\n+---+-----+-----------+------+----------+\nonly showing top 3 rows\n\nroot\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- hire_date: string (nullable = true)\n\n['id', 'name', 'department', 'salary', 'hire_date']\nTotal rows: 5\n"}, {"name": "stderr", "output_type": "stream", "text": "25/06/26 14:08:17 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-------+------------------+-----+-----------+------------------+----------+\n|summary|                id| name| department|            salary| hire_date|\n+-------+------------------+-----+-----------+------------------+----------+\n|  count|                 5|    5|          5|                 5|         5|\n|   mean|               3.0| NULL|       NULL|           73000.0|      NULL|\n| stddev|1.5811388300841898| NULL|       NULL|10368.220676663861|      NULL|\n|    min|                 1|David|Engineering|             60000|2018-11-05|\n|    max|                 5|Sarah|  Marketing|             85000|2022-02-28|\n+-------+------------------+-----+-----------+------------------+----------+\n\n"}], "source": "# Show first few rows\ndf_employees.show()\ndf_employees.show(3)  # Show only 3 rows\n\n# Print schema\ndf_employees.printSchema()\n\n# Get column names\nprint(df_employees.columns)\n\n# Count rows\nprint(f\"Total rows: {df_employees.count()}\")\n\n# Describe statistics\ndf_employees.describe().show()"}, {"cell_type": "markdown", "id": "37c51c4a-db4e-4610-a91f-c889a9e4b712", "metadata": {}, "source": "### Selecting Columns"}, {"cell_type": "code", "execution_count": 16, "id": "25025a96-b4f1-4d9b-a270-0fa6a2815b8c", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+------+\n| name|salary|\n+-----+------+\n| John| 75000|\n|Sarah| 65000|\n| Mike| 80000|\n| Lisa| 60000|\n|David| 85000|\n+-----+------+\n\n+-----+------+------+\n| name|salary| bonus|\n+-----+------+------+\n| John| 75000|7500.0|\n|Sarah| 65000|6500.0|\n| Mike| 80000|8000.0|\n| Lisa| 60000|6000.0|\n|David| 85000|8500.0|\n+-----+------+------+\n\n"}], "source": "# Select specific columns\ndf_employees.select(\"name\", \"salary\").show()\n\n# Select with column expressions\ndf_employees.select(\n    col(\"name\"),\n    col(\"salary\"),\n    (col(\"salary\") * 0.1).alias(\"bonus\")\n).show()"}, {"cell_type": "markdown", "id": "7a11bf1c-f176-4b23-942f-881caafafe98", "metadata": {}, "source": "## 3. Filtering Data"}, {"cell_type": "code", "execution_count": 17, "id": "98989568-0405-48a5-b520-1fa7124c9973", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----+-----------+------+----------+\n| id| name| department|salary| hire_date|\n+---+-----+-----------+------+----------+\n|  1| John|Engineering| 75000|2020-01-15|\n|  3| Mike|Engineering| 80000|2021-06-10|\n|  5|David|Engineering| 85000|2022-02-28|\n+---+-----+-----------+------+----------+\n\n+---+-----+-----------+------+----------+\n| id| name| department|salary| hire_date|\n+---+-----+-----------+------+----------+\n|  1| John|Engineering| 75000|2020-01-15|\n|  3| Mike|Engineering| 80000|2021-06-10|\n|  5|David|Engineering| 85000|2022-02-28|\n+---+-----+-----------+------+----------+\n\n+---+-----+-----------+------+----------+\n| id| name| department|salary| hire_date|\n+---+-----+-----------+------+----------+\n|  1| John|Engineering| 75000|2020-01-15|\n|  3| Mike|Engineering| 80000|2021-06-10|\n|  4| Lisa|         HR| 60000|2018-11-05|\n|  5|David|Engineering| 85000|2022-02-28|\n+---+-----+-----------+------+----------+\n\n+---+-----+----------+------+----------+\n| id| name|department|salary| hire_date|\n+---+-----+----------+------+----------+\n|  2|Sarah| Marketing| 65000|2019-03-22|\n+---+-----+----------+------+----------+\n\n"}], "source": "# Simple filter\ndf_employees.filter(col(\"salary\") > 70000).show()\n\n# Multiple conditions\ndf_employees.filter(\n    (col(\"salary\") > 70000) & \n    (col(\"department\") == \"Engineering\")\n).show()\n\n# Using where (alias for filter)\ndf_employees.where(col(\"department\").isin([\"Engineering\", \"HR\"])).show()\n\n# String operations\ndf_employees.filter(col(\"name\").startswith(\"S\")).show()"}, {"cell_type": "markdown", "id": "a5f263ac-5f77-45b5-897d-58fe2c0b1fe2", "metadata": {}, "source": "## 4. Adding and Modifying Columns"}, {"cell_type": "code", "execution_count": 18, "id": "c31c1154-5c64-4b4b-a7b5-978bf5393eca", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----+-----------+------+----------+------+\n| id| name| department|salary| hire_date| bonus|\n+---+-----+-----------+------+----------+------+\n|  1| John|Engineering| 75000|2020-01-15|7500.0|\n|  2|Sarah|  Marketing| 65000|2019-03-22|6500.0|\n|  3| Mike|Engineering| 80000|2021-06-10|8000.0|\n|  4| Lisa|         HR| 60000|2018-11-05|6000.0|\n|  5|David|Engineering| 85000|2022-02-28|8500.0|\n+---+-----+-----------+------+----------+------+\n\n+---+-----+-----------+------+----------+------+------------------+---------+\n| id| name| department|salary| hire_date| bonus|total_compensation|is_senior|\n+---+-----+-----------+------+----------+------+------------------+---------+\n|  1| John|Engineering| 75000|2020-01-15|7500.0|           82500.0|     true|\n|  2|Sarah|  Marketing| 65000|2019-03-22|6500.0|           71500.0|    false|\n|  3| Mike|Engineering| 80000|2021-06-10|8000.0|           88000.0|     true|\n|  4| Lisa|         HR| 60000|2018-11-05|6000.0|           66000.0|    false|\n|  5|David|Engineering| 85000|2022-02-28|8500.0|           93500.0|     true|\n+---+-----+-----------+------+----------+------+------------------+---------+\n\n+---+-----+-----------+------+----------+\n| id| name|       dept|salary| hire_date|\n+---+-----+-----------+------+----------+\n|  1| John|Engineering| 75000|2020-01-15|\n|  2|Sarah|  Marketing| 65000|2019-03-22|\n|  3| Mike|Engineering| 80000|2021-06-10|\n|  4| Lisa|         HR| 60000|2018-11-05|\n|  5|David|Engineering| 85000|2022-02-28|\n+---+-----+-----------+------+----------+\n\n"}], "source": "# Add new columns\ndf_with_bonus = df_employees.withColumn(\"bonus\", col(\"salary\") * 0.1)\ndf_with_bonus.show()\n\n# Multiple new columns\ndf_enhanced = df_employees \\\n    .withColumn(\"bonus\", col(\"salary\") * 0.1) \\\n    .withColumn(\"total_compensation\", col(\"salary\") + col(\"bonus\")) \\\n    .withColumn(\"is_senior\", col(\"salary\") > 70000)\n\ndf_enhanced.show()\n\n# Rename columns\ndf_renamed = df_employees.withColumnRenamed(\"department\", \"dept\")\ndf_renamed.show()"}, {"cell_type": "markdown", "id": "ad7c28b3-c362-4f92-be39-edf539f24442", "metadata": {}, "source": "## 5. Grouping and Aggregations"}, {"cell_type": "code", "execution_count": 19, "id": "a2713419-1e7b-423c-887c-25abf846724d", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+--------------+----------+----------+----------+\n| department|employee_count|avg_salary|max_salary|min_salary|\n+-----------+--------------+----------+----------+----------+\n|Engineering|             3|   80000.0|     85000|     75000|\n|  Marketing|             1|   65000.0|     65000|     65000|\n|         HR|             1|   60000.0|     60000|     60000|\n+-----------+--------------+----------+----------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 35:=============================>                            (1 + 1) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+--------+-----------+------------+\n| department| project|total_hours|people_count|\n+-----------+--------+-----------+------------+\n|Engineering|ProjectB|         35|           1|\n|Engineering|ProjectA|         90|           2|\n|  Marketing|ProjectC|         45|           1|\n+-----------+--------+-----------+------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Group by department\ndept_stats = df_employees.groupBy(\"department\") \\\n    .agg(\n        count(\"*\").alias(\"employee_count\"),\n        avg(\"salary\").alias(\"avg_salary\"),\n        max(\"salary\").alias(\"max_salary\"),\n        min(\"salary\").alias(\"min_salary\")\n    )\n\ndept_stats.show()\n\n# Multiple grouping columns (create sample data first)\nprojects_data = [\n    (1, \"John\", \"Engineering\", \"ProjectA\", 40),\n    (1, \"John\", \"Engineering\", \"ProjectB\", 35),\n    (2, \"Sarah\", \"Marketing\", \"ProjectC\", 45),\n    (3, \"Mike\", \"Engineering\", \"ProjectA\", 50)\n]\n\ndf_projects = spark.createDataFrame(projects_data, \n    [\"id\", \"name\", \"department\", \"project\", \"hours\"])\n\nproject_summary = df_projects.groupBy(\"department\", \"project\") \\\n    .agg(\n        sum(\"hours\").alias(\"total_hours\"),\n        count(\"*\").alias(\"people_count\")\n    )\n\nproject_summary.show()"}, {"cell_type": "markdown", "id": "b5275121-c166-4aaa-898f-6609a53bbccf", "metadata": {}, "source": "## 7. Joins"}, {"cell_type": "code", "execution_count": 20, "id": "ab3355dd-b225-4cb4-9900-a52680952aa7", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+---+-----+------+----------+----------+----------+\n| department| id| name|salary| hire_date|   manager|  location|\n+-----------+---+-----+------+----------+----------+----------+\n|Engineering|  3| Mike| 80000|2021-06-10|John Smith|Building A|\n|Engineering|  5|David| 85000|2022-02-28|John Smith|Building A|\n|Engineering|  1| John| 75000|2020-01-15|John Smith|Building A|\n|         HR|  4| Lisa| 60000|2018-11-05|Bob Wilson|Building C|\n|  Marketing|  2|Sarah| 65000|2019-03-22|  Jane Doe|Building B|\n+-----------+---+-----+------+----------+----------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+---+-----+------+----------+----------+----------+\n| department| id| name|salary| hire_date|   manager|  location|\n+-----------+---+-----+------+----------+----------+----------+\n|Engineering|  1| John| 75000|2020-01-15|John Smith|Building A|\n|  Marketing|  2|Sarah| 65000|2019-03-22|  Jane Doe|Building B|\n|Engineering|  3| Mike| 80000|2021-06-10|John Smith|Building A|\n|Engineering|  5|David| 85000|2022-02-28|John Smith|Building A|\n|         HR|  4| Lisa| 60000|2018-11-05|Bob Wilson|Building C|\n+-----------+---+-----+------+----------+----------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 50:>                                                         (0 + 2) / 2]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---+-----+-----------+------+----------+----------+----------+\n| id| name| department|salary| hire_date|   manager|  location|\n+---+-----+-----------+------+----------+----------+----------+\n|  3| Mike|Engineering| 80000|2021-06-10|John Smith|Building A|\n|  5|David|Engineering| 85000|2022-02-28|John Smith|Building A|\n|  1| John|Engineering| 75000|2020-01-15|John Smith|Building A|\n|  4| Lisa|         HR| 60000|2018-11-05|Bob Wilson|Building C|\n|  2|Sarah|  Marketing| 65000|2019-03-22|  Jane Doe|Building B|\n+---+-----+-----------+------+----------+----------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Create second DataFrame for joining\ndepartments_data = [\n    (\"Engineering\", \"John Smith\", \"Building A\"),\n    (\"Marketing\", \"Jane Doe\", \"Building B\"),\n    (\"HR\", \"Bob Wilson\", \"Building C\")\n]\n\ndf_departments = spark.createDataFrame(departments_data, \n    [\"department\", \"manager\", \"location\"])\n\n# Inner join\njoined_df = df_employees.join(df_departments, \"department\", \"inner\")\njoined_df.show()\n\n# Left join\nleft_joined = df_employees.join(df_departments, \"department\", \"left\")\nleft_joined.show()\n\n# Join with explicit conditions\nexplicit_join = df_employees.join(\n    df_departments, \n    df_employees.department == df_departments.department,\n    \"inner\"\n).select(\n    df_employees[\"*\"],\n    df_departments.manager,\n    df_departments.location\n)\nexplicit_join.show()"}, {"cell_type": "markdown", "id": "fa06896a-aa6f-4439-b5a6-1cbdad71e9a2", "metadata": {}, "source": "## 8. Window Functions"}, {"cell_type": "code", "execution_count": 21, "id": "c5d248cd-1861-4953-b91d-214185ec2104", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----+-----------+------+----------+----------+----+----------+\n| id| name| department|salary| hire_date|row_number|rank|dense_rank|\n+---+-----+-----------+------+----------+----------+----+----------+\n|  5|David|Engineering| 85000|2022-02-28|         1|   1|         1|\n|  3| Mike|Engineering| 80000|2021-06-10|         2|   2|         2|\n|  1| John|Engineering| 75000|2020-01-15|         3|   3|         3|\n|  4| Lisa|         HR| 60000|2018-11-05|         1|   1|         1|\n|  2|Sarah|  Marketing| 65000|2019-03-22|         1|   1|         1|\n+---+-----+-----------+------+----------+----------+----+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "[Stage 59:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+---+-----+-----------+------+----------+-------------+\n| id| name| department|salary| hire_date|running_total|\n+---+-----+-----------+------+----------+-------------+\n|  1| John|Engineering| 75000|2020-01-15|        75000|\n|  3| Mike|Engineering| 80000|2021-06-10|       155000|\n|  5|David|Engineering| 85000|2022-02-28|       240000|\n|  4| Lisa|         HR| 60000|2018-11-05|        60000|\n|  2|Sarah|  Marketing| 65000|2019-03-22|        65000|\n+---+-----+-----------+------+----------+-------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql.window import Window\n\n# Define window specification\nwindow_spec = Window.partitionBy(\"department\").orderBy(col(\"salary\").desc())\n\n# Add row numbers and rankings\ndf_with_window = df_employees \\\n    .withColumn(\"row_number\", row_number().over(window_spec)) \\\n    .withColumn(\"rank\", rank().over(window_spec)) \\\n    .withColumn(\"dense_rank\", dense_rank().over(window_spec))\n\ndf_with_window.show()\n\n# Running totals\nwindow_running = Window.partitionBy(\"department\").orderBy(\"salary\") \\\n    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\ndf_running_total = df_employees \\\n    .withColumn(\"running_total\", sum(\"salary\").over(window_running))\n\ndf_running_total.show()"}, {"cell_type": "markdown", "id": "6ae888d0-fb41-4c48-895f-9fca5fdc57c3", "metadata": {}, "source": "## 9. SQL Queries on DataFrames"}, {"cell_type": "code", "execution_count": 22, "id": "63edd338-16fb-4697-b738-cb915e183e2f", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----------+--------------+----------+\n| department|employee_count|avg_salary|\n+-----------+--------------+----------+\n|Engineering|             3|   80000.0|\n|  Marketing|             1|   65000.0|\n|         HR|             1|   60000.0|\n+-----------+--------------+----------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+------+-----------+----------+----------+\n| name|salary| department|   manager|  location|\n+-----+------+-----------+----------+----------+\n|David| 85000|Engineering|John Smith|Building A|\n| Mike| 80000|Engineering|John Smith|Building A|\n| John| 75000|Engineering|John Smith|Building A|\n+-----+------+-----------+----------+----------+\n\n+---+-----+-----------+------+----------+\n| id| name| department|salary| hire_date|\n+---+-----+-----------+------+----------+\n|  1| John|Engineering| 75000|2020-01-15|\n|  3| Mike|Engineering| 80000|2021-06-10|\n|  5|David|Engineering| 85000|2022-02-28|\n+---+-----+-----------+------+----------+\n\n"}], "source": "# Register DataFrame as temporary view\ndf_employees.createOrReplaceTempView(\"employees\")\ndf_departments.createOrReplaceTempView(\"departments\")\n\n# Execute SQL queries\nresult1 = spark.sql(\"\"\"\n    SELECT department, \n           COUNT(*) as employee_count,\n           AVG(salary) as avg_salary\n    FROM employees \n    GROUP BY department\n    ORDER BY avg_salary DESC\n\"\"\")\nresult1.show()\n\n# Complex SQL with joins\nresult2 = spark.sql(\"\"\"\n    SELECT e.name, e.salary, e.department, d.manager, d.location\n    FROM employees e\n    JOIN departments d ON e.department = d.department\n    WHERE e.salary > 70000\n    ORDER BY e.salary DESC\n\"\"\")\nresult2.show()\n\n# Subqueries\nresult3 = spark.sql(\"\"\"\n    SELECT * FROM employees\n    WHERE salary > (\n        SELECT AVG(salary) FROM employees\n    )\n\"\"\")\nresult3.show()"}, {"cell_type": "markdown", "id": "3d31d248-9d1c-4d8e-b71a-1969d0a8e305", "metadata": {}, "source": "## 10. Working with Dates"}, {"cell_type": "code", "execution_count": 23, "id": "559152ff-6e6d-4835-9219-b64d73b0b4dd", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "root\n |-- id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- department: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- hire_date: date (nullable = true)\n\n+---+-----+-----------+------+----------+---------+----------+---------------+\n| id| name| department|salary| hire_date|hire_year|hire_month|days_since_hire|\n+---+-----+-----------+------+----------+---------+----------+---------------+\n|  1| John|Engineering| 75000|2020-01-15|     2020|         1|           1989|\n|  2|Sarah|  Marketing| 65000|2019-03-22|     2019|         3|           2288|\n|  3| Mike|Engineering| 80000|2021-06-10|     2021|         6|           1477|\n|  4| Lisa|         HR| 60000|2018-11-05|     2018|        11|           2425|\n|  5|David|Engineering| 85000|2022-02-28|     2022|         2|           1214|\n+---+-----+-----------+------+----------+---------+----------+---------------+\n\n"}], "source": "# Convert string to date\ndf_with_dates = df_employees \\\n    .withColumn(\"hire_date\", to_date(col(\"hire_date\"), \"yyyy-MM-dd\"))\n\ndf_with_dates.printSchema()\n\n# Date operations\ndf_date_ops = df_with_dates \\\n    .withColumn(\"hire_year\", year(\"hire_date\")) \\\n    .withColumn(\"hire_month\", month(\"hire_date\")) \\\n    .withColumn(\"days_since_hire\", datediff(current_date(), \"hire_date\"))\n\ndf_date_ops.show()"}, {"cell_type": "markdown", "id": "b21e502f-a69a-47ce-a805-5d2083b861e5", "metadata": {}, "source": "## 11. Handling Null Values"}, {"cell_type": "code", "execution_count": 24, "id": "fd3ea8b3-2bcb-43e4-b7fb-6bee7d22125f", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----+-----------+------+\n| id| name| department|salary|\n+---+-----+-----------+------+\n|  1| John|Engineering| 75000|\n|  2|Sarah|       NULL| 65000|\n|  3| NULL|Engineering|  NULL|\n|  4| Lisa|         HR| 60000|\n+---+-----+-----------+------+\n\n+---+----+-----------+------+\n| id|name| department|salary|\n+---+----+-----------+------+\n|  1|John|Engineering| 75000|\n|  4|Lisa|         HR| 60000|\n+---+----+-----------+------+\n\n+---+-----+-----------+------+\n| id| name| department|salary|\n+---+-----+-----------+------+\n|  1| John|Engineering| 75000|\n|  2|Sarah|       NULL| 65000|\n|  4| Lisa|         HR| 60000|\n+---+-----+-----------+------+\n\n+---+-------+-----------+------+\n| id|   name| department|salary|\n+---+-------+-----------+------+\n|  1|   John|Engineering| 75000|\n|  2|  Sarah| Unassigned| 65000|\n|  3|Unknown|Engineering|     0|\n|  4|   Lisa|         HR| 60000|\n+---+-------+-----------+------+\n\n"}], "source": "# Create data with nulls\ndata_with_nulls = [\n    (1, \"John\", \"Engineering\", 75000),\n    (2, \"Sarah\", None, 65000),\n    (3, None, \"Engineering\", None),\n    (4, \"Lisa\", \"HR\", 60000)\n]\n\ndf_nulls = spark.createDataFrame(data_with_nulls, \n    [\"id\", \"name\", \"department\", \"salary\"])\n\n# Check for nulls\ndf_nulls.show()\n\n# Drop rows with any null\ndf_nulls.dropna().show()\n\n# Drop rows with null in specific columns\ndf_nulls.dropna(subset=[\"name\"]).show()\n\n# Fill nulls\ndf_filled = df_nulls.fillna({\n    \"name\": \"Unknown\",\n    \"department\": \"Unassigned\",\n    \"salary\": 0\n})\ndf_filled.show()"}, {"cell_type": "markdown", "id": "a294ee58-9a96-4d57-b0bf-2639c9c8730e", "metadata": {}, "source": "## 12. Advanced Operations"}, {"cell_type": "markdown", "id": "92841687-38b5-475f-95c9-eebb1991be58", "metadata": {}, "source": "### Pivot Tables"}, {"cell_type": "code", "execution_count": 25, "id": "3ff0fd7d-d16a-42c5-a169-2d0170697257", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------+---------+-----+-----+\n|quarter|  product|North|South|\n+-------+---------+-----+-----+\n|     Q1|Product B|  150|  130|\n|     Q2|Product A|  120|   90|\n|     Q1|Product A|  100|   80|\n+-------+---------+-----+-----+\n\n"}], "source": "# Create sample sales data\nsales_data = [\n    (\"Q1\", \"North\", \"Product A\", 100),\n    (\"Q1\", \"South\", \"Product A\", 80),\n    (\"Q2\", \"North\", \"Product A\", 120),\n    (\"Q2\", \"South\", \"Product A\", 90),\n    (\"Q1\", \"North\", \"Product B\", 150),\n    (\"Q1\", \"South\", \"Product B\", 130)\n]\n\ndf_sales = spark.createDataFrame(sales_data, \n    [\"quarter\", \"region\", \"product\", \"sales\"])\n\n# Pivot\npivoted = df_sales.groupBy(\"quarter\", \"product\") \\\n    .pivot(\"region\") \\\n    .sum(\"sales\")\n\npivoted.show()"}, {"cell_type": "markdown", "id": "25337368-004b-483f-8f6e-562db0571ebf", "metadata": {}, "source": "### User Defined Functions (UDFs)"}, {"cell_type": "code", "execution_count": 26, "id": "42dc41f4-249f-42e7-8632-dc5cbbaadc31", "metadata": {"tags": []}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}, {"name": "stdout", "output_type": "stream", "text": "+---+-----+-----------+------+----------+---------------+\n| id| name| department|salary| hire_date|salary_category|\n+---+-----+-----------+------+----------+---------------+\n|  1| John|Engineering| 75000|2020-01-15|         Medium|\n|  2|Sarah|  Marketing| 65000|2019-03-22|            Low|\n|  3| Mike|Engineering| 80000|2021-06-10|           High|\n|  4| Lisa|         HR| 60000|2018-11-05|            Low|\n|  5|David|Engineering| 85000|2022-02-28|           High|\n+---+-----+-----------+------+----------+---------------+\n\n"}], "source": "from pyspark.sql.functions import udf\n\n# Define UDF\ndef categorize_salary(salary):\n    if salary >= 80000:\n        return \"High\"\n    elif salary >= 70000:\n        return \"Medium\"\n    else:\n        return \"Low\"\n\n# Register UDF\ncategorize_udf = udf(categorize_salary, StringType())\n\n# Use UDF\ndf_categorized = df_employees \\\n    .withColumn(\"salary_category\", categorize_udf(col(\"salary\")))\n\ndf_categorized.show()"}, {"cell_type": "markdown", "id": "23142c27-b99b-48d4-94b4-c89ae3db240a", "metadata": {}, "source": "## Practice Exercises\n### Try these exercises to reinforce your learning:\n\n#### Exercise 1: Find the employee with the highest salary in each department\n#### Exercise 2: Calculate the percentage of total salary each employee represents within their department\n#### Exercise 3: Find departments where the average salary is above the company average\n#### Exercise 4: Create a report showing month-over-month hiring trends\n#### Exercise 5: Find employees whose names contain specific patterns"}, {"cell_type": "code", "execution_count": 27, "id": "f080c68d-02f3-4ded-99c4-3d85748ac3b9", "metadata": {"tags": []}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+---+-----+-----------+------+----------+\n| id| name| department|salary| hire_date|\n+---+-----+-----------+------+----------+\n|  5|David|Engineering| 85000|2022-02-28|\n|  4| Lisa|         HR| 60000|2018-11-05|\n|  2|Sarah|  Marketing| 65000|2019-03-22|\n+---+-----+-----------+------+----------+\n\n+---+-----+-----------+------+----------+----------+-----------------+\n| id| name| department|salary| hire_date|dept_total|salary_percentage|\n+---+-----+-----------+------+----------+----------+-----------------+\n|  1| John|Engineering| 75000|2020-01-15|    240000|            31.25|\n|  3| Mike|Engineering| 80000|2021-06-10|    240000|            33.33|\n|  5|David|Engineering| 85000|2022-02-28|    240000|            35.42|\n|  4| Lisa|         HR| 60000|2018-11-05|     60000|            100.0|\n|  2|Sarah|  Marketing| 65000|2019-03-22|     65000|            100.0|\n+---+-----+-----------+------+----------+----------+-----------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "# Exercise 1: Highest salary per department\nwindow_max = Window.partitionBy(\"department\")\nexercise1 = df_employees \\\n    .withColumn(\"max_dept_salary\", max(\"salary\").over(window_max)) \\\n    .filter(col(\"salary\") == col(\"max_dept_salary\")) \\\n    .drop(\"max_dept_salary\")\n\nexercise1.show()\n\n# Exercise 2: Salary percentage per department\nwindow_dept = Window.partitionBy(\"department\")\nexercise2 = df_employees \\\n    .withColumn(\"dept_total\", sum(\"salary\").over(window_dept)) \\\n    .withColumn(\"salary_percentage\", \n                round((col(\"salary\") / col(\"dept_total\")) * 100, 2))\n\nexercise2.show()"}, {"cell_type": "markdown", "id": "f420243d-411f-40b6-8def-3f437a7d6e92", "metadata": {}, "source": "## Performance Tips"}, {"cell_type": "markdown", "id": "24007dba-fe7d-48ec-a69e-b1f92f23d06b", "metadata": {}, "source": "### 1 - Use caching for DataFrames you'll reuse:"}, {"cell_type": "code", "execution_count": null, "id": "c9a86bd4-2427-4fda-8ecd-c8e10f25ee7b", "metadata": {}, "outputs": [], "source": "df_employees.cache()\ndf_employees.count()  # Triggers caching"}, {"cell_type": "markdown", "id": "e412ab19-bcfb-4568-9c4a-cc3bcb5e0911", "metadata": {}, "source": "### 2 - Partition your data when reading from files:"}, {"cell_type": "code", "execution_count": null, "id": "1312f683-0397-43d1-bfc4-53b0ad9626c5", "metadata": {}, "outputs": [], "source": "# When reading from files\ndf = spark.read.option(\"multiline\", \"true\").json(\"path/to/files\")"}, {"cell_type": "markdown", "id": "63b05798-21e2-499b-81af-abd0f30de820", "metadata": {}, "source": "### 3 - Use appropriate file formats (Parquet is usually best):"}, {"cell_type": "code", "execution_count": null, "id": "74f0b444-de10-4bf1-8702-190b152da18e", "metadata": {}, "outputs": [], "source": "# Write to Parquet\ndf_employees.write.mode(\"overwrite\").parquet(\"employees.parquet\")\n\n# Read from Parquet\ndf_read = spark.read.parquet(\"employees.parquet\")"}, {"cell_type": "code", "execution_count": null, "id": "ecdfa90f-57f2-4d63-8778-d09bfbcb5cd0", "metadata": {"tags": []}, "outputs": [], "source": "#stop spark\nspark.stop()"}, {"cell_type": "code", "execution_count": null, "id": "6253452e-6fe8-46e3-b2ba-5ae810ba030a", "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.11.8"}}, "nbformat": 4, "nbformat_minor": 5}