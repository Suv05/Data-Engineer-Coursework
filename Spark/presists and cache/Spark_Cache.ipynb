{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30cc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum, count, col, when\n",
    "\n",
    "spark = SparkSession.builder.appName(\"PersistExample\").getOrCreate()\n",
    "\n",
    "\n",
    "# Cache example\n",
    "df = spark.read.parquet(\"transactions.parquet\")\n",
    "\n",
    "# Apply transformations\n",
    "expensive_df = df.groupBy(\"customer_id\").agg(\n",
    "    sum(\"amount\").alias(\"total_spent\"),\n",
    "    count(\"*\").alias(\"transaction_count\")\n",
    ").filter(col(\"total_spent\") > 1000)\n",
    "\n",
    "# Cache the result since we'll analyze it further\n",
    "expensive_df.cache()\n",
    "\n",
    "# Multiple operations that benefit from caching\n",
    "high_value_customers = expensive_df.filter(col(\"total_spent\") > 10000)\n",
    "customer_segments = expensive_df.withColumn(\"segment\", \n",
    "    when(col(\"total_spent\") > 5000, \"Premium\")\n",
    "    .when(col(\"total_spent\") > 2000, \"Gold\")\n",
    "    .otherwise(\"Silver\"))\n",
    "\n",
    "# Show results\n",
    "high_value_customers.show()\n",
    "customer_segments.groupBy(\"segment\").count().show()\n",
    "\n",
    "# Clean up\n",
    "expensive_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5fd63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
